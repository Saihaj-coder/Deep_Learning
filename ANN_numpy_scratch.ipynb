{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311b0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c54810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/DELL 5410/Downloads/ANN_Data/ANN_Data/data1.csv', header=None)\n",
    "labels = pd.read_csv('C:/Users/DELL 5410/Downloads/ANN_Data/ANN_Data/label1.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5250a914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6   \\\n",
      "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
      "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
      "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
      "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
      "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
      "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
      "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
      "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
      "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
      "\n",
      "             7         8         9         10        11        12        13  \\\n",
      "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
      "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
      "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
      "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
      "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
      "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
      "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
      "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
      "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
      "\n",
      "             14        15        16        17        18        19  \n",
      "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
      "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
      "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
      "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
      "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
      "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
      "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
      "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
      "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
      "\n",
      "[20000 rows x 20 columns]\n",
      "       0\n",
      "0      3\n",
      "1      2\n",
      "2      2\n",
      "3      3\n",
      "4      3\n",
      "...   ..\n",
      "19995  3\n",
      "19996  1\n",
      "19997  2\n",
      "19998  2\n",
      "19999  0\n",
      "\n",
      "[20000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235d480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 20)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "data=data.values\n",
    "npLabels=labels.values\n",
    "print(data.shape)\n",
    "print(npLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a88df77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(npLabels))\n",
    "print(np.min(npLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba766f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1219f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "num=np.max(npLabels)+1\n",
    "oneHot=np.eye(num)[npLabels]\n",
    "print(oneHot.ndim)\n",
    "print(oneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74766bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "oneHotRe=oneHot.reshape(20000, 5)\n",
    "print(oneHotRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab985bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData,testDataDash=train_test_split(data,train_size=0.8,test_size=0.2,shuffle=False)\n",
    "trainLabel,testLabelDash=train_test_split(oneHotRe,train_size=0.8,test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2009e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validData,testData=train_test_split(testDataDash,train_size=0.5,test_size=0.5,shuffle=False)\n",
    "validLabel,testLabel=train_test_split(testLabelDash,train_size=0.5,test_size=0.5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eadfe57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c760068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "b1=np.zeros(h)\n",
    "b2=np.zeros(5)\n",
    "print(b1)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b8a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.random.normal(0, 1, (20,h))\n",
    "w2=np.random.normal(0, 1, (h,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fef2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actFunc(data,choice):\n",
    "\n",
    "    if(choice==1):\n",
    "        return np.tanh(data)\n",
    "    elif(choice==2):\n",
    "        numerator=np.exp(data)\n",
    "        return numerator/np.sum(numerator,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf7edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09caaa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "605814f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunc(y,t):\n",
    "    return -(t*(np.log(y))+(1-t)*np.log(1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ae9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwFunc(data):\n",
    "    a0=data\n",
    "    z1=np.dot(a0,w1)+b1\n",
    "    a1=actFunc(z1,1)\n",
    "    z2=np.dot(a1,w2)+b2\n",
    "    a2=actFunc(z2,2)\n",
    "    return a0,z1,a1,z2,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7874a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(y,t,size):\n",
    "    maxData=np.argmax(y,axis=1)\n",
    "    maxLabel=np.argmax(t,axis=1)\n",
    "    compare=np.equal(maxData,maxLabel)\n",
    "    count=np.sum(compare)\n",
    "    return (count/size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14840442",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab03fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 1.2684190587066415\n",
      "Training accuracy 72.61875\n",
      "Training Cost 1.1497088771908723\n",
      "Training accuracy 75.61875\n",
      "Training Cost 1.069462487808354\n",
      "Training accuracy 77.575\n",
      "Training Cost 1.0091352344114122\n",
      "Training accuracy 79.1125\n",
      "Training Cost 0.9582399416280536\n",
      "Training accuracy 80.5875\n",
      "Training Cost 0.9143857120033938\n",
      "Training accuracy 82.0625\n",
      "Training Cost 0.8736699245827508\n",
      "Training accuracy 83.28125\n",
      "Training Cost 0.837775392000299\n",
      "Training accuracy 84.3375\n",
      "Training Cost 0.8069767720094756\n",
      "Training accuracy 84.925\n",
      "Training Cost 0.784100182444046\n",
      "Training accuracy 85.32499999999999\n",
      "Training Cost 0.7648091940020231\n",
      "Training accuracy 85.64375\n",
      "Training Cost 0.7471054654460126\n",
      "Training accuracy 86.20625\n",
      "Training Cost 0.7319146370164304\n",
      "Training accuracy 86.59375\n",
      "Training Cost 0.7215851405801694\n",
      "Training accuracy 86.85625\n",
      "Training Cost 0.7178432497975539\n",
      "Training accuracy 86.91875\n",
      "Training Cost 0.7119789027669433\n",
      "Training accuracy 87.03125\n",
      "Training Cost 0.710845152374972\n",
      "Training accuracy 86.99375\n",
      "Training Cost 0.7101755686424384\n",
      "Training accuracy 86.9\n",
      "Training Cost 0.7075286296140582\n",
      "Training accuracy 86.9\n",
      "Training Cost 0.686218636653458\n",
      "Training accuracy 87.26875\n",
      "Training Cost 0.6679490805036981\n",
      "Training accuracy 87.4\n",
      "Training Cost 0.6751176960582279\n",
      "Training accuracy 87.3125\n",
      "Training Cost 0.6453396808719731\n",
      "Training accuracy 87.94999999999999\n",
      "Training Cost 0.666971672740794\n",
      "Training accuracy 87.54375\n",
      "Training Cost 0.6488652033745869\n",
      "Training accuracy 87.98125\n",
      "Training Cost 0.6373540739257331\n",
      "Training accuracy 88.2\n",
      "Training Cost 0.626890951976941\n",
      "Training accuracy 88.48125\n",
      "Training Cost 0.6170640275335258\n",
      "Training accuracy 88.64375\n",
      "Training Cost 0.6079388218145048\n",
      "Training accuracy 88.8375\n",
      "Training Cost 0.5987812979326799\n",
      "Training accuracy 89.06875\n",
      "Training Cost 0.5898061552454685\n",
      "Training accuracy 89.24375\n",
      "Training Cost 0.5815646447283629\n",
      "Training accuracy 89.35\n",
      "Training Cost 0.574473558279499\n",
      "Training accuracy 89.50625\n",
      "Training Cost 0.5682224536502501\n",
      "Training accuracy 89.625\n",
      "Training Cost 0.5627573754891987\n",
      "Training accuracy 89.76875\n",
      "Training Cost 0.5580688516437107\n",
      "Training accuracy 89.89375\n",
      "Training Cost 0.5540204384412926\n",
      "Training accuracy 89.95625000000001\n",
      "Training Cost 0.5504649939633873\n",
      "Training accuracy 90.025\n",
      "Training Cost 0.5472934209492906\n",
      "Training accuracy 90.1375\n",
      "Training Cost 0.5444319373725931\n",
      "Training accuracy 90.18125\n",
      "Training Cost 0.5418299733508802\n",
      "Training accuracy 90.26875\n",
      "Training Cost 0.5394489953966215\n",
      "Training accuracy 90.2875\n",
      "Training Cost 0.5372537300726208\n",
      "Training accuracy 90.30625\n",
      "Training Cost 0.5352085577579376\n",
      "Training accuracy 90.36874999999999\n",
      "Training Cost 0.5332809921078177\n",
      "Training accuracy 90.38125000000001\n",
      "Training Cost 0.531447414942822\n",
      "Training accuracy 90.43124999999999\n",
      "Training Cost 0.5296952160986091\n",
      "Training accuracy 90.46875\n",
      "Training Cost 0.5280203211020725\n",
      "Training accuracy 90.51875\n",
      "Training Cost 0.5264229776689102\n",
      "Training accuracy 90.5\n",
      "Training Cost 0.5249065659410801\n",
      "Training accuracy 90.58125\n",
      "Training Cost 0.5234778499750886\n",
      "Training accuracy 90.60625\n",
      "Training Cost 0.5221423280851477\n",
      "Training accuracy 90.6375\n",
      "Training Cost 0.5208981185570339\n",
      "Training accuracy 90.73125\n",
      "Training Cost 0.5197341141327492\n",
      "Training accuracy 90.71875\n",
      "Training Cost 0.5186322098459386\n",
      "Training accuracy 90.79374999999999\n",
      "Training Cost 0.5175724626092479\n",
      "Training accuracy 90.78125\n",
      "Training Cost 0.5165400395398788\n",
      "Training accuracy 90.825\n",
      "Training Cost 0.5155294790044654\n",
      "Training accuracy 90.80625\n",
      "Training Cost 0.5145417171870201\n",
      "Training accuracy 90.8375\n",
      "Training Cost 0.5135766279418793\n",
      "Training accuracy 90.8625\n",
      "Training Cost 0.5126271257821279\n",
      "Training accuracy 90.85625\n",
      "Training Cost 0.5116776342440633\n",
      "Training accuracy 90.8375\n",
      "Training Cost 0.5107058501855917\n",
      "Training accuracy 90.8875\n",
      "Training Cost 0.5096851918345201\n",
      "Training accuracy 90.90625\n",
      "Training Cost 0.5085868440399564\n",
      "Training accuracy 90.9\n",
      "Training Cost 0.5073819106924069\n",
      "Training accuracy 90.89375\n",
      "Training Cost 0.5060449455332919\n",
      "Training accuracy 90.93124999999999\n",
      "Training Cost 0.5045664189174406\n",
      "Training accuracy 90.95625000000001\n",
      "Training Cost 0.5029647107863042\n",
      "Training accuracy 91.00625\n",
      "Training Cost 0.5012640669344043\n",
      "Training accuracy 91.03125\n",
      "Training Cost 0.4994964122183598\n",
      "Training accuracy 91.07499999999999\n",
      "Training Cost 0.49772877668741133\n",
      "Training accuracy 91.06875\n",
      "Training Cost 0.4960512457086773\n",
      "Training accuracy 91.1125\n",
      "Training Cost 0.4945547438994994\n",
      "Training accuracy 91.13125\n",
      "Training Cost 0.4932927764689382\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4922469195087704\n",
      "Training accuracy 91.1875\n",
      "Training Cost 0.4913357647234757\n",
      "Training accuracy 91.18125\n",
      "Training Cost 0.4904663878473165\n",
      "Training accuracy 91.24375\n",
      "Training Cost 0.489591193377412\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.488713521761962\n",
      "Training accuracy 91.28125\n",
      "Training Cost 0.4878769852512479\n",
      "Training accuracy 91.3\n",
      "Training Cost 0.4871446521595621\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4865545270629847\n",
      "Training accuracy 91.325\n",
      "Training Cost 0.48609666681771\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.48572991746338656\n",
      "Training accuracy 91.29374999999999\n",
      "Training Cost 0.485413101135119\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.48512347397977557\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.48485446149117123\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4846060736183475\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.48437857310116683\n",
      "Training accuracy 91.36875\n",
      "Training Cost 0.4841704491326937\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.4839786662551827\n",
      "Training accuracy 91.325\n",
      "Training Cost 0.4837997337180311\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.48363097928066257\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.48347150172234216\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.4833221164802659\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.4831841741592158\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4830581720133853\n",
      "Training accuracy 91.30625\n",
      "Training Cost 0.482943237733361\n",
      "Training accuracy 91.325\n",
      "Training Cost 0.4828376080557925\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.48273940528210096\n",
      "Training accuracy 91.325\n",
      "Training Cost 0.48264714296220956\n",
      "Training accuracy 91.31875000000001\n",
      "Training Cost 0.4825598934184428\n",
      "Training accuracy 91.33125\n",
      "Training Cost 0.4824772674538464\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.482399319093726\n",
      "Training accuracy 91.3125\n",
      "Training Cost 0.48232641850287106\n",
      "Training accuracy 91.33125\n",
      "Training Cost 0.48225910897335916\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.48219796296667067\n",
      "Training accuracy 91.325\n",
      "Training Cost 0.48214345639646267\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.482095879726647\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.4820552963686781\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.4820215462129706\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4819942811402518\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.48197301491820244\n",
      "Training accuracy 91.36875\n",
      "Training Cost 0.48195717226708923\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.48194612747781596\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.4819392277922348\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.4819357994011253\n",
      "Training accuracy 91.3375\n",
      "Training Cost 0.4819351359087752\n",
      "Training accuracy 91.34375\n",
      "Training Cost 0.48193647270722395\n",
      "Training accuracy 91.35\n",
      "Training Cost 0.4819389560919573\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.4819416204390002\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.4819433867958449\n",
      "Training accuracy 91.375\n",
      "Training Cost 0.48194309044197886\n",
      "Training accuracy 91.35625\n",
      "Training Cost 0.4819395353822876\n",
      "Training accuracy 91.3625\n",
      "Training Cost 0.481931564463656\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.4819181287813265\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.4818983412550022\n",
      "Training accuracy 91.3875\n",
      "Training Cost 0.48187150586498617\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.4818371227107806\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4817948757810819\n",
      "Training accuracy 91.425\n",
      "Training Cost 0.4817446127404985\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4816863244437477\n",
      "Training accuracy 91.4125\n",
      "Training Cost 0.4816201283510333\n",
      "Training accuracy 91.40625\n",
      "Training Cost 0.4815462566936628\n",
      "Training accuracy 91.41875\n",
      "Training Cost 0.48146504829508935\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4813769424930305\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.4812824741378906\n",
      "Training accuracy 91.43124999999999\n",
      "Training Cost 0.48118226955344096\n",
      "Training accuracy 91.45\n",
      "Training Cost 0.48107704417984737\n",
      "Training accuracy 91.4625\n",
      "Training Cost 0.48096760306836434\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.48085484498373343\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4807397682011135\n",
      "Training accuracy 91.4875\n",
      "Training Cost 0.48062346804439765\n",
      "Training accuracy 91.5125\n",
      "Training Cost 0.4805070992090707\n",
      "Training accuracy 91.54375\n",
      "Training Cost 0.48039175392311934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 91.54375\n",
      "Training Cost 0.4802782090880242\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.48016657912446065\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.4800560797109806\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4799451751349328\n",
      "Training accuracy 91.5\n",
      "Training Cost 0.4798321372601832\n",
      "Training accuracy 91.525\n",
      "Training Cost 0.47971569139568815\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47959538529339324\n",
      "Training accuracy 91.48125\n",
      "Training Cost 0.4794715796586299\n",
      "Training accuracy 91.475\n",
      "Training Cost 0.4793451934018577\n",
      "Training accuracy 91.49374999999999\n",
      "Training Cost 0.47921738927010776\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4790893196623373\n",
      "Training accuracy 91.53125\n",
      "Training Cost 0.4789619705820637\n",
      "Training accuracy 91.51875000000001\n",
      "Training Cost 0.4788360920997689\n",
      "Training accuracy 91.53750000000001\n",
      "Training Cost 0.47871218731374476\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.47859053370414073\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.4784712185722508\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.47835417779607203\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.4782392325246865\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.4781261216998974\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4780145300087489\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.47790411160412927\n",
      "Training accuracy 91.55624999999999\n",
      "Training Cost 0.47779451009538243\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.4776853751733563\n",
      "Training accuracy 91.58125\n",
      "Training Cost 0.47757637596891733\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.4774672109825349\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.4773576142799801\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.47724735771560095\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.47713624925032866\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.47702412791126214\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.47691085643662084\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4767963129792566\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.4766803832851011\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.47656295452615016\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.4764439115617907\n",
      "Training accuracy 91.61875\n",
      "Training Cost 0.4763231359655312\n",
      "Training accuracy 91.60625\n",
      "Training Cost 0.4762005077804103\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.47607590968434865\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.47594923304314124\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.47582038518031994\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.47568929708265933\n",
      "Training accuracy 91.5625\n",
      "Training Cost 0.47555593067667223\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.47542028474636777\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.4752823985132826\n",
      "Training accuracy 91.56875\n",
      "Training Cost 0.47514235188289816\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4750002614264618\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4748562713958986\n",
      "Training accuracy 91.55\n",
      "Training Cost 0.47471053954553893\n",
      "Training accuracy 91.57499999999999\n",
      "Training Cost 0.47456321828023984\n",
      "Training accuracy 91.59375\n",
      "Training Cost 0.47441443256196353\n",
      "Training accuracy 91.5875\n",
      "Training Cost 0.4742642568499457\n",
      "Training accuracy 91.60000000000001\n",
      "Training Cost 0.47411269384216786\n",
      "Training accuracy 91.6125\n",
      "Training Cost 0.4739596577421075\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.4738049641891106\n",
      "Training accuracy 91.625\n",
      "Training Cost 0.4736483280078415\n",
      "Training accuracy 91.64999999999999\n",
      "Training Cost 0.47348936876378855\n",
      "Training accuracy 91.66250000000001\n",
      "Training Cost 0.4733276229926911\n",
      "Training accuracy 91.675\n",
      "Training Cost 0.47316256114678457\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4729936069661981\n",
      "Training accuracy 91.6875\n",
      "Training Cost 0.4728201571929239\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.4726416001715217\n",
      "Training accuracy 91.66875\n",
      "Training Cost 0.47245733267083007\n",
      "Training accuracy 91.68125\n",
      "Training Cost 0.47226677488007063\n",
      "Training accuracy 91.70625\n",
      "Training Cost 0.47206938355574085\n",
      "Training accuracy 91.71875\n",
      "Training Cost 0.47186466227902746\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.47165216574536595\n",
      "Training accuracy 91.73125\n",
      "Training Cost 0.4714314935635311\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.4712022726708368\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.4709641421745357\n",
      "Training accuracy 91.70625\n",
      "Training Cost 0.47071677728221817\n",
      "Training accuracy 91.725\n",
      "Training Cost 0.47045998124889427\n",
      "Training accuracy 91.73750000000001\n",
      "Training Cost 0.47019373958797844\n",
      "Training accuracy 91.76875\n",
      "Training Cost 0.4699178376721564\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.4696306085282685\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.46932722423921097\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.46899861909134394\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.468630467202002\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.46819966430156856\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.46766825621633906\n",
      "Training accuracy 91.77499999999999\n",
      "Training Cost 0.46698315365198484\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.46610726644974343\n",
      "Training accuracy 91.80625\n",
      "Training Cost 0.4651067386483582\n",
      "Training accuracy 91.7625\n",
      "Training Cost 0.4641716084408433\n",
      "Training accuracy 91.75\n",
      "Training Cost 0.46342812112975207\n",
      "Training accuracy 91.78125\n",
      "Training Cost 0.46286640020763437\n",
      "Training accuracy 91.79375\n",
      "Training Cost 0.46243713499885586\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46209802398614885\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4618180319832439\n",
      "Training accuracy 91.8\n",
      "Training Cost 0.46157646089948323\n",
      "Training accuracy 91.8375\n",
      "Training Cost 0.46136021433226176\n",
      "Training accuracy 91.825\n",
      "Training Cost 0.4611610772650387\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.46097380190472553\n",
      "Training accuracy 91.81875000000001\n",
      "Training Cost 0.4607949115470041\n",
      "Training accuracy 91.8125\n",
      "Training Cost 0.46062200445920787\n",
      "Training accuracy 91.83125\n",
      "Training Cost 0.46045337076420306\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.46028779414891435\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.4601244551125305\n",
      "Training accuracy 91.875\n",
      "Training Cost 0.4599628796755242\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.45980289466048646\n",
      "Training accuracy 91.85624999999999\n",
      "Training Cost 0.4596445664504305\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.45948811839019993\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45933383915077264\n",
      "Training accuracy 91.8875\n",
      "Training Cost 0.45918200284787075\n",
      "Training accuracy 91.875\n",
      "Training Cost 0.45903281774506927\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.4588864083305817\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4587428239603941\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4586020617705017\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4584640922684274\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.45832887983098364\n",
      "Training accuracy 91.88125000000001\n",
      "Training Cost 0.4581963943577877\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45806661331666004\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.45793951546354533\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.45781506908311875\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45769321873457675\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4575738747592497\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.45745690881164375\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4573421566493262\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4572294272271055\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.45711851568337236\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4570092174389488\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4569013411141252\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4567947188150412\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.45668921314412764\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.45658472087168683\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.45648117356212414\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.45637853564548236\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4562768005264496\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4561759853626964\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.45607612513152085\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4559772665398085\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45587946222496784\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4557827655622518\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.45568722626177643\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4555928868295814\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.45549977989449403\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4554079263665769\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.45531733438284244\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.455227998995896\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.45513990255706066\n",
      "Training accuracy 91.91875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.4550530157299972\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4549672990446597\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4548827048716235\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4547991796724425\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4547166663698602\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.45463510668540175\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4545544433097917\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4544746217996583\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.45439559212701613\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.45431730984155994\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4542397368365931\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4541628417354909\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4540865999359701\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45401099336366046\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.45393600999451256\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.45386164320766625\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.45378789102718337\n",
      "Training accuracy 91.8875\n",
      "Training Cost 0.4537147553035466\n",
      "Training accuracy 91.8875\n",
      "Training Cost 0.4536422408754866\n",
      "Training accuracy 91.88125000000001\n",
      "Training Cost 0.4535703547411281\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.45349910525627496\n",
      "Training accuracy 91.86875\n",
      "Training Cost 0.4534285013682668\n",
      "Training accuracy 91.875\n",
      "Training Cost 0.4533585518872298\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.4532892647932789\n",
      "Training accuracy 91.85624999999999\n",
      "Training Cost 0.45322064657839023\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.4531527016249001\n",
      "Training accuracy 91.8625\n",
      "Training Cost 0.4530854316280981\n",
      "Training accuracy 91.875\n",
      "Training Cost 0.45301883507694357\n",
      "Training accuracy 91.88125000000001\n",
      "Training Cost 0.4529529068129824\n",
      "Training accuracy 91.8875\n",
      "Training Cost 0.45288763769136337\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.4528230143678958\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.45275901923133116\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.45269563049036143\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45263282241120045\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.452570565686156\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4525088278992628\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4524475740450568\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4523867670537407\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4523263682819079\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4522663379424783\n",
      "Training accuracy 91.9\n",
      "Training Cost 0.4522066354683587\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4521472198276575\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.4520880498292188\n",
      "Training accuracy 91.89375\n",
      "Training Cost 0.4520290844711997\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4519702833892026\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.451911607453259\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.45185301954655116\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.45179448553726864\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.45173597543375593\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.45167746469702913\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4516189356764077\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.451560379132115\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4515017958074998\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45144319800419686\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4513846110875365\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.451326074803202\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.45126764422494353\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4512093900932221\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4511513982705582\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.45109376805771406\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.451036609204539\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4509800376113489\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4509241699274676\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.45086911746678004\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.45081498001982273\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.45076184020235166\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.45070975892405596\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.45065877240495095\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.45060889095319595\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.45056009949688036\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.45051235967631065\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4504656131798554\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4504197859570724\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.45037479296562005\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4503305431894735\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.45028694478095027\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4502439102953\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.45020136206513556\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.450159237759271\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4501174960427191\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4500761219714335\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.45003513133411055\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4499945727122008\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4499545258263549\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.44991509511343625\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.44987639862520623\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4498385539788002\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4498016643942385\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4497658079425494\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.44973103184331903\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.44969735180049486\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4496647550327895\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4496332053163194\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4496026487246063\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44957301928474747\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4495442441478682\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44951624806323137\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.44948895703717673\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4494623011241121\n",
      "Training accuracy 91.90625\n",
      "Training Cost 0.4494362163614514\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44941064591908786\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44938554057694213\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4493608586670725\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.449336565621413\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4493126332571905\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.44928903891473515\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4492657645411864\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.44924279579170373\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4492201211992805\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44919773144626346\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44917561875569206\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4491537764086447\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44913219838469814\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4491108791160269\n",
      "Training accuracy 91.91875\n",
      "Training Cost 0.4490898133411915\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.4490689960419035\n",
      "Training accuracy 91.9125\n",
      "Training Cost 0.44904842244465226\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4490280880687491\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.4490079888028282\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.44898812099298757\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.44896848152741065\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4489490679044184\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4489298782734254\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4489109114411954\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44889216683907907\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.44887364445049543\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44885534470164234\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4488372683220441\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44881941618473886\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4488017891382876\n",
      "Training accuracy 91.96875\n",
      "Training Cost 0.44878438784399144\n",
      "Training accuracy 91.96875\n",
      "Training Cost 0.44876721263147124\n",
      "Training accuracy 91.96875\n",
      "Training Cost 0.44875026338401947\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.448733539462045\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4487170396689042\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44870076225901656\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4486847049840267\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.44866886516947774\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.44865323981235583\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4486378256891184\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4486226194643204\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.44860761779143693\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.44859281739957124\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.44857821516205515\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4485638081451646\n",
      "Training accuracy 91.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost 0.4485495936370632\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4485355691585143\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4485217324578547\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.44850808149321747\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.44849461440512123\n",
      "Training accuracy 91.925\n",
      "Training Cost 0.44848132948240205\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4484682251241423\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.44845529979984194\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4484425520096265\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4484299802458716\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4484175829572298\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4484053585157319\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4483933051873689\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4483814211063624\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.44836970425317785\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.44835815243623334\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4483467632771767\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44833553419955813\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.44832446242068985\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4483135449464651\n",
      "Training accuracy 91.93124999999999\n",
      "Training Cost 0.4483027785688942\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4482921598661061\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4482816852045589\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4482713507431996\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.448261152439312\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4482510860557971\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44824114716962904\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4482313311812461\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44822163332463827\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4482120486779153\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4482025721741455\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44819319861228396\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44818392266801943\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44817473890439324\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44816564178206475\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4481566256691118\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4481476848502827\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4481388135356245\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4481300058684344\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.44812125593249386\n",
      "Training accuracy 91.9375\n",
      "Training Cost 0.4481125577585591\n",
      "Training accuracy 91.94375000000001\n",
      "Training Cost 0.4481039053300899\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44809529258821196\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44808671343591544\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44807816174149756\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4480696313412649\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4480611160415157\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4480526096198252\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.4480441058256622\n",
      "Training accuracy 91.95\n",
      "Training Cost 0.44803559838036416\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4480270809765047\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.44801854727668033\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.44800999091175275\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4480014054785707\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4479927845372022\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.447984121607697\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44797541016639425\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44796664364178607\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.44795781540992835\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4479489187893875\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44793994703568546\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4479308933351955\n",
      "Training accuracy 91.96875\n",
      "Training Cost 0.44792175079841257\n",
      "Training accuracy 91.96875\n",
      "Training Cost 0.4479125124525035\n",
      "Training accuracy 91.96875\n",
      "Training Cost 0.44790317123301593\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.4478937199745949\n",
      "Training accuracy 91.9625\n",
      "Training Cost 0.44788415140053633\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.4478744581109743\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44786463256948694\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44785466708788146\n",
      "Training accuracy 91.95625\n",
      "Training Cost 0.44784455380891974\n",
      "Training accuracy 91.95\n",
      "validation accuracy\n",
      "90.5\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "for epoch in range(epochs):\n",
    "    for iteration in range(int(len(trainData)/batch)):\n",
    "        a0,z1,a1,z2,a2=forwFunc(trainData[iteration*batch:(iteration+1)*batch,:])\n",
    "        y=a2\n",
    "        labelBatch=trainLabel[iteration*batch:(iteration+1)*batch,:]\n",
    "        del2=(y-labelBatch)\n",
    "        del1=np.dot(del2,w2.T)*(1 - a1**2)\n",
    "        dcdw2=np.dot(a1.T,del2)\n",
    "        dcdw1=np.dot(a0.T,del1)\n",
    "        dcdb1=np.sum(del1,axis=0)\n",
    "        dcdb2=np.sum(del2,axis=0)\n",
    "        w1=w1-alpha*dcdw1\n",
    "        w2=w2-alpha*dcdw2\n",
    "        b2=b2-alpha*dcdb2\n",
    "        b1=b1-alpha*dcdb1\n",
    "    a0,z1,a1,z2,a2=forwFunc(trainData)\n",
    "    print(\"Training Cost\",(np.sum(costFunc(a2,trainLabel)))/16000.0)\n",
    "    cost.append(np.sum(costFunc(a2,trainLabel))/16000.0)\n",
    "    print(\"Training accuracy\",Acc(a2,trainLabel,len(trainLabel)))\n",
    "    \n",
    "va0,vz1,va1,vz2,va2=forwFunc(validData)\n",
    "vOutput=va2\n",
    "accuracy = Acc(vOutput,validLabel,2000.0)\n",
    "print(\"validation accuracy\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24d8e923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19458e7fb20>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaf9a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
